{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReviewerResponseTable.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blindauth/abstention_experiments/blob/master/ReviewerResponseTable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PP_1glqtoye",
        "colab_type": "code",
        "outputId": "afef1192-132c-4b15-9375-fe7132fb8a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "!rm -r abstention\n",
        "!pip uninstall abstention\n",
        "!git clone https://github.com/blindauth/abstention.git\n",
        "!pip install abstention/\n",
        "![[ -e abstention_experiments ]] || git clone https://github.com/blindauth/abstention_experiments.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'abstention': No such file or directory\n",
            "\u001b[33mWARNING: Skipping abstention as it is not installed.\u001b[0m\n",
            "Cloning into 'abstention'...\n",
            "remote: Enumerating objects: 135, done.\u001b[K\n",
            "remote: Counting objects: 100% (135/135), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 135 (delta 81), reused 95 (delta 41), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (135/135), 61.46 KiB | 4.10 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n",
            "Processing ./abstention\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.6/dist-packages (from abstention==0.1.2.1) (1.16.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from abstention==0.1.2.1) (0.21.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from abstention==0.1.2.1) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->abstention==0.1.2.1) (0.13.2)\n",
            "Building wheels for collected packages: abstention\n",
            "  Building wheel for abstention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pjq4sot3/wheels/60/8e/d2/9e9ca02e7b5f76bfda2e2daa6dcbe42c19095c502ccb653729\n",
            "Successfully built abstention\n",
            "Installing collected packages: abstention\n",
            "Successfully installed abstention-0.1.2.1\n",
            "Cloning into 'abstention_experiments'...\n",
            "remote: Enumerating objects: 1169, done.\u001b[K\n",
            "remote: Counting objects: 100% (1169/1169), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1163/1163), done.\u001b[K\n",
            "remote: Total 4107 (delta 72), reused 1048 (delta 5), pack-reused 2938\u001b[K\n",
            "Receiving objects: 100% (4107/4107), 289.89 MiB | 48.45 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n",
            "Checking out files: 100% (3895/3895), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qtgbb96kOv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "\n",
        "task0_abstention_results =\\\n",
        "  json.loads(gzip.open(\n",
        "    \"abstention_experiments/hema/heme_task0_abstention_results.json.gz\").read())\n",
        "task1_abstention_results =\\\n",
        "  json.loads(gzip.open(\n",
        "    \"abstention_experiments/hema/heme_task1_abstention_results.json.gz\").read())\n",
        "catdog_abstention_results =\\\n",
        "  json.loads(gzip.open(\n",
        "    \"abstention_experiments/catdog/catdog_abstention_results.json.gz\").read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KPStlv82uqz",
        "colab_type": "code",
        "outputId": "04de8419-1c02-48c2-b26e-71652150c38b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "from abstention.figure_making_utils import (\n",
        "    wilcox_srs, get_ustats_mat,\n",
        "    get_tied_top_and_worst_methods)\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "datasetname_to_dataset = {\n",
        "    'catdog': (catdog_abstention_results['metricname_to_abstfrac_to_abstnadapt_to_perf'],\n",
        "               catdog_abstention_results['metricname_to_adaptname_to_baselineperfs']),\n",
        "    'hemetask0': (task0_abstention_results['metricname_to_abstfrac_to_abstncalib_to_perf'],\n",
        "                  task0_abstention_results['metricname_to_baselineperfs']),\n",
        "    'hemetask1': (task1_abstention_results['metricname_to_abstfrac_to_abstncalib_to_perf'],\n",
        "                  task1_abstention_results['metricname_to_baselineperfs'])\n",
        "}\n",
        "\n",
        "nicename_orders = [\n",
        "    \"$\\Delta$Metric (Ours)\",\n",
        "    #\"Est $\\Delta$Metric (Ours)\",\n",
        "    \"JS Div. (Ours)\",\n",
        "    #\"JS Div. (Ours)\",\n",
        "    \"Dist. from 0.5\",\n",
        "    \"Dist. from 0.5\",\n",
        "    \"Fumera et al.\",\n",
        "    \"Fumera et al.\",\n",
        "    \"Test-time Drop.\",\n",
        "    \"Test-time Drop.\"\n",
        "]\n",
        "\n",
        "yesnoyesno = ['Y','Y','N','Y','N','Y','N','Y','N']\n",
        "\n",
        "def get_method_orders(pc, metric, adapt):\n",
        "  if adapt:\n",
        "    possuffix='EM'\n",
        "    negsuffix='None'\n",
        "  else:\n",
        "    possuffix='platt'\n",
        "    negsuffix='none'   \n",
        "  return [\n",
        "      'mcwindowdelta_'+metric+'_'+pc+'pc:'+possuffix if metric==\"tpratfpr\" else 'estwindowdelta_'+metric+'_'+pc+'pc:'+possuffix,\n",
        "      #'mcwindowdelta_'+metric+'_'+pc+'pc:'+negsuffix,\n",
        "      'js:'+possuffix,\n",
        "      #'js:'+negsuffix,\n",
        "      'distfrom0.5:'+possuffix,\n",
        "      'distfrom0.5:'+negsuffix,\n",
        "      'fumera_'+metric+'_'+pc+'pc:'+possuffix,\n",
        "      'fumera_'+metric+'_'+pc+'pc:'+negsuffix,\n",
        "      'uncertainty:'+possuffix,\n",
        "      'uncertainty:'+negsuffix, \n",
        "  ]\n",
        "  \n",
        "\n",
        "catdogtpr_abstfrac_to_methodorders = {\n",
        "    '0.3': get_method_orders('30', 'tpratfpr', adapt=True),\n",
        "    '0.15': get_method_orders('15', 'tpratfpr', adapt=True)\n",
        "}\n",
        "hemeauprc_abstfrac_to_methodorders = {\n",
        "    '0.3': get_method_orders('30', 'auprc', adapt=False),\n",
        "    '0.15': get_method_orders('15', 'auprc', adapt=False)\n",
        "}\n",
        "\n",
        "dataset_and_metricname_combos = [\n",
        "    ('hemetask1', 'auprc', hemeauprc_abstfrac_to_methodorders),\n",
        "    ('hemetask0', 'auprc', hemeauprc_abstfrac_to_methodorders),\n",
        "    ('catdog', 'tpratfpr_0.01', catdogtpr_abstfrac_to_methodorders),\n",
        "]\n",
        "\n",
        "abstfracs = ['0.15', '0.3']\n",
        "\n",
        "\n",
        "def stdderr(vals):\n",
        "  return np.std(vals, ddof=1)/np.sqrt(len(vals))\n",
        "\n",
        "def convert_method_to_perfs_to_ranks(method_to_perfs, method_names):\n",
        "  n_samples = len(method_to_perfs[method_names[0]])\n",
        "  method_to_ranks = defaultdict(list)\n",
        "  for sampleidx in range(n_samples):\n",
        "    method_and_perf_tuples = [\n",
        "        (method_name, method_to_perfs[method_name][sampleidx])\n",
        "         for method_name in method_names]\n",
        "    sorted_method_and_perfs_tuples = sorted(method_and_perf_tuples,\n",
        "                                            key=lambda x: -x[1])\n",
        "    for i,(methodname,perf) in enumerate(sorted_method_and_perfs_tuples):\n",
        "      method_to_ranks[methodname].append(i+1)\n",
        "  return method_to_ranks\n",
        "  \n",
        "  \n",
        "\n",
        "datasetnmetricname_to_abstfrac_to_bestmethods = defaultdict(dict)\n",
        "datasetnmetricname_to_abstfrac_to_methodranks = defaultdict(dict)\n",
        "for (datasetname, metricname, abstfrac_to_methodorders) in dataset_and_metricname_combos:  \n",
        "  metricname_to_abstfrac_to_method_to_perf, metricname_to_baselineperfs =\\\n",
        "    datasetname_to_dataset[datasetname]\n",
        "  #identify the best methods\n",
        "  for abstfrac in abstfracs:\n",
        "    method_names = abstfrac_to_methodorders[abstfrac]\n",
        "    nsamples = len(list(metricname_to_abstfrac_to_method_to_perf\n",
        "                        [metricname][abstfrac][method_names[0]]))\n",
        "    method_to_perfs = metricname_to_abstfrac_to_method_to_perf[metricname][abstfrac]\n",
        "    method_to_ranks = convert_method_to_perfs_to_ranks(\n",
        "                        method_to_perfs=method_to_perfs,\n",
        "                        method_names=method_names)\n",
        "    datasetnmetricname_to_abstfrac_to_methodranks[\n",
        "        datasetname+\":\"+metricname][abstfrac] = method_to_ranks\n",
        "    ustats_mat = get_ustats_mat(\n",
        "        method_to_perfs=method_to_perfs,\n",
        "        method_names=method_names,\n",
        "        max_ustat=nsamples*(nsamples+1)/2 )\n",
        "    tied_top_methods, tied_worst_methods =(\n",
        "        get_tied_top_and_worst_methods(\n",
        "            ustats_mat,\n",
        "            method_names,\n",
        "            #0.05 threshold for one-sided test from normal approx when N=30,\n",
        "            #from https://www.oreilly.com/library/view/nonparametric-statistics-a/9781118840429/bapp02.xhtml\n",
        "            threshold=151\n",
        "        ))\n",
        "    tied_top_methods = [method_names[x] for x in tied_top_methods]\n",
        "    print(\"bestmethods\",metricname, abstfrac, tied_top_methods)\n",
        "    datasetnmetricname_to_abstfrac_to_bestmethods[\n",
        "        datasetname+\":\"+metricname][abstfrac] = set(tied_top_methods)\n",
        "    \n",
        "\n",
        "\n",
        "#generate the latex table\n",
        "thetable = (\"\"\"\n",
        "\\\\begin{table*}[!h]\n",
        "\\\\tiny\n",
        "\\\\begin{center}\n",
        "\\\\begin{tabular}{ | c c | c c | c c | c c | }\n",
        "\\\\hline\n",
        "\\\\multicolumn{1}{| c }{ } & A/ & \\\\multicolumn{2}{ c |}{AuPRC (Leukemia Stem Cells) (0.695 $\\\\pm$ 0.002)}\"\"\"\n",
        "                       +\"\"\"& \\\\multicolumn{2}{ c |}{AuPRC (pre-leukemic HSCs) (0.734 $\\\\pm$ 0.002) }\"\"\"\n",
        "                       +\"\"\"& \\\\multicolumn{2}{ c |}{Cat v Dog Sens. @ 99\\\\% Spec. (0.332 $\\\\pm$ 0.006)}\\\\\\\\\n",
        "\\\\multicolumn{1}{| c}{Method} & \\\\multicolumn{1}{c |}{C?} & @30\\\\% Abst; avg. rank & @15\\\\% Abst; avg. rank & @30\\\\% Abst; avg. rank & @15\\\\% Abst; avg. rank & @30\\\\% Abst; avg. rank & @15\\\\% Abst; avg. rank\\\\\\\\ \\\\hline\n",
        "\"\"\")\n",
        "for rowidx in range(len(nicename_orders)):\n",
        "  nicemethodname = nicename_orders[rowidx]\n",
        "  yesorno = yesnoyesno[rowidx]\n",
        "  thetable += nicemethodname\n",
        "  thetable += \" & \"+yesorno\n",
        "  for (datasetname, metricname, abstfrac_to_methodorders) in dataset_and_metricname_combos: \n",
        "    (metricname_to_abstfrac_to_methodname_to_perf,\n",
        "     metricname_to_baselineperfs)  = datasetname_to_dataset[datasetname]\n",
        "    #index by adapt-or-not, if needed\n",
        "    if (isinstance(metricname_to_baselineperfs[metricname], list)):\n",
        "      baseperfs = metricname_to_baselineperfs[metricname]\n",
        "    else:\n",
        "      baseperfs = metricname_to_baselineperfs[metricname][\n",
        "                   abstfrac_to_methodorders['0.3'][rowidx].split(\":\")[1]]    \n",
        "    print(np.round(np.mean(baseperfs),4), np.round(stdderr(baseperfs),3))\n",
        "    #thetable += (\" & \"+str(np.round(np.mean(baseperfs),4))\n",
        "                 #+\"$\\\\pm$\"+str(np.round(stdderr(baseperfs),4))\n",
        "    #            )\n",
        "    for abstfrac in ['0.3', '0.15']:\n",
        "      methodname = abstfrac_to_methodorders[abstfrac][rowidx]\n",
        "      isbest = (methodname in\n",
        "        datasetnmetricname_to_abstfrac_to_bestmethods[\n",
        "            datasetname+\":\"+metricname][abstfrac])\n",
        "      \n",
        "      ranks = datasetnmetricname_to_abstfrac_to_methodranks[\n",
        "          datasetname+\":\"+metricname][abstfrac][methodname]\n",
        "      \n",
        "      methodperfs = metricname_to_abstfrac_to_methodname_to_perf[\n",
        "        metricname][abstfrac][methodname]\n",
        "      thetable += \" & \"\n",
        "      if (isbest):\n",
        "        thetable += \"\\\\textbf{\"\n",
        "      thetable += (str(np.round(np.mean(methodperfs),3))\n",
        "                   +\"$\\pm$\"+str(np.round(stdderr(methodperfs),3))\n",
        "                   +\"; \"+str(np.round(np.mean(ranks),1))\n",
        "                        +\"$\\pm$\"+str(np.round(stdderr(ranks),1)))\n",
        "      if (isbest):\n",
        "        thetable += \"}\"\n",
        "  thetable += \"\\\\\\\\ \\\\hline\\n\"\n",
        "  \n",
        "thetable += \"\"\"\\\\end{tabular}\n",
        "\\\\end{center}\n",
        "\\\\caption{\\\\textbf{}. }\n",
        "\\\\label{tab:reviewers}\n",
        "\\\\end{table*}\n",
        "\"\"\"\n",
        "print(\"\\n\\n\"+thetable)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bestmethods auprc 0.15 ['estwindowdelta_auprc_15pc:platt']\n",
            "bestmethods auprc 0.3 ['estwindowdelta_auprc_30pc:platt']\n",
            "bestmethods auprc 0.15 ['estwindowdelta_auprc_15pc:platt']\n",
            "bestmethods auprc 0.3 ['estwindowdelta_auprc_30pc:platt']\n",
            "bestmethods tpratfpr_0.01 0.15 ['mcwindowdelta_tpratfpr_15pc:EM']\n",
            "bestmethods tpratfpr_0.01 0.3 ['mcwindowdelta_tpratfpr_30pc:EM', 'fumera_tpratfpr_30pc:None']\n",
            "0.6946 0.002\n",
            "0.7341 0.002\n",
            "0.3318 0.006\n",
            "0.6946 0.002\n",
            "0.7341 0.002\n",
            "0.3318 0.006\n",
            "0.6946 0.002\n",
            "0.7341 0.002\n",
            "0.3318 0.006\n",
            "0.6946 0.002\n",
            "0.7341 0.002\n",
            "0.3318 0.006\n",
            "0.6946 0.002\n",
            "0.7341 0.002\n",
            "0.3318 0.006\n",
            "0.6946 0.002\n",
            "0.7341 0.002\n",
            "0.3318 0.006\n",
            "0.6946 0.002\n",
            "0.7341 0.002\n",
            "0.3318 0.006\n",
            "0.6946 0.002\n",
            "0.7341 0.002\n",
            "0.3318 0.006\n",
            "\n",
            "\n",
            "\n",
            "\\begin{table*}[!h]\n",
            "\\tiny\n",
            "\\begin{center}\n",
            "\\begin{tabular}{ | c c | c c | c c | c c | }\n",
            "\\hline\n",
            "\\multicolumn{1}{| c }{ } & A/ & \\multicolumn{2}{ c |}{AuPRC (Leukemia Stem Cells) (0.695 $\\pm$ 0.002)}& \\multicolumn{2}{ c |}{AuPRC (pre-leukemic HSCs) (0.734 $\\pm$ 0.002) }& \\multicolumn{2}{ c |}{Cat v Dog Sens. @ 99\\% Spec. (0.332 $\\pm$ 0.006)}\\\\\n",
            "\\multicolumn{1}{| c}{Method} & \\multicolumn{1}{c |}{C?} & @30\\% Abst; avg. rank & @15\\% Abst; avg. rank & @30\\% Abst; avg. rank & @15\\% Abst; avg. rank & @30\\% Abst; avg. rank & @15\\% Abst; avg. rank\\\\ \\hline\n",
            "$\\Delta$Metric (Ours) & Y & \\textbf{0.752$\\pm$0.002; 1.7$\\pm$0.2} & \\textbf{0.722$\\pm$0.002; 1.2$\\pm$0.1} & \\textbf{0.803$\\pm$0.002; 1.5$\\pm$0.1} & \\textbf{0.767$\\pm$0.002; 1.1$\\pm$0.0} & \\textbf{0.613$\\pm$0.011; 1.7$\\pm$0.1} & \\textbf{0.46$\\pm$0.008; 1.3$\\pm$0.1}\\\\ \\hline\n",
            "JS Div. (Ours) & Y & 0.752$\\pm$0.002; 1.9$\\pm$0.1 & 0.722$\\pm$0.002; 2.1$\\pm$0.1 & 0.803$\\pm$0.002; 1.9$\\pm$0.1 & 0.767$\\pm$0.002; 1.9$\\pm$0.0 & 0.404$\\pm$0.008; 7.0$\\pm$0.1 & 0.366$\\pm$0.006; 6.4$\\pm$0.1\\\\ \\hline\n",
            "Dist. from 0.5 & N & 0.718$\\pm$0.002; 6.1$\\pm$0.1 & 0.705$\\pm$0.002; 6.4$\\pm$0.1 & 0.771$\\pm$0.002; 5.6$\\pm$0.1 & 0.749$\\pm$0.002; 5.7$\\pm$0.2 & 0.523$\\pm$0.01; 3.5$\\pm$0.1 & 0.412$\\pm$0.008; 2.9$\\pm$0.1\\\\ \\hline\n",
            "Dist. from 0.5 & Y & 0.731$\\pm$0.006; 5.1$\\pm$0.3 & 0.711$\\pm$0.003; 4.8$\\pm$0.3 & 0.769$\\pm$0.006; 5.6$\\pm$0.1 & 0.749$\\pm$0.004; 5.5$\\pm$0.2 & 0.413$\\pm$0.008; 5.9$\\pm$0.1 & 0.366$\\pm$0.006; 6.2$\\pm$0.1\\\\ \\hline\n",
            "Fumera et al. & N & 0.75$\\pm$0.002; 3.5$\\pm$0.1 & 0.716$\\pm$0.002; 4.2$\\pm$0.2 & 0.801$\\pm$0.002; 3.5$\\pm$0.1 & 0.76$\\pm$0.002; 3.8$\\pm$0.1 & 0.529$\\pm$0.01; 3.4$\\pm$0.2 & 0.445$\\pm$0.008; 1.9$\\pm$0.2\\\\ \\hline\n",
            "Fumera et al. & Y & 0.749$\\pm$0.003; 3.5$\\pm$0.2 & 0.717$\\pm$0.003; 3.6$\\pm$0.1 & 0.799$\\pm$0.002; 3.0$\\pm$0.2 & 0.759$\\pm$0.003; 3.6$\\pm$0.1 & \\textbf{0.615$\\pm$0.01; 1.7$\\pm$0.1} & 0.402$\\pm$0.008; 4.0$\\pm$0.1\\\\ \\hline\n",
            "Test-time Drop. & N & 0.662$\\pm$0.012; 7.2$\\pm$0.2 & 0.685$\\pm$0.005; 6.7$\\pm$0.3 & 0.681$\\pm$0.013; 7.3$\\pm$0.1 & 0.711$\\pm$0.006; 7.2$\\pm$0.2 & 0.452$\\pm$0.014; 5.2$\\pm$0.2 & 0.375$\\pm$0.008; 5.3$\\pm$0.2\\\\ \\hline\n",
            "Test-time Drop. & Y & 0.69$\\pm$0.008; 7.1$\\pm$0.2 & 0.695$\\pm$0.005; 6.9$\\pm$0.3 & 0.672$\\pm$0.013; 7.4$\\pm$0.1 & 0.707$\\pm$0.008; 7.3$\\pm$0.2 & 0.375$\\pm$0.01; 7.7$\\pm$0.1 & 0.349$\\pm$0.007; 7.9$\\pm$0.1\\\\ \\hline\n",
            "\\end{tabular}\n",
            "\\end{center}\n",
            "\\caption{\\textbf{}. }\n",
            "\\label{tab:reviewers}\n",
            "\\end{table*}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRCp8hHrtBSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}